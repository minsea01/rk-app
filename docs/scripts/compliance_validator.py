#!/usr/bin/env python3
"""
RK3588å·¥ä¸šæ£€æµ‹ç³»ç»Ÿè¾¾æ ‡éªŒè¯è„šæœ¬
é€šè¿‡å®é™…æ•°æ®æµ‹è¯•éªŒè¯æ˜¯å¦è¾¾åˆ°é¡¹ç›®æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
"""

import os
import sys
import time
import json
import subprocess
import socket
import threading
import cv2
import numpy as np
from datetime import datetime
from pathlib import Path

class ComplianceValidator:
    """é¡¹ç›®è¾¾æ ‡éªŒè¯å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.compliance_status = {}
        self.test_start_time = time.time()
        
        # é¡¹ç›®è¦æ±‚æ ‡å‡†
        self.requirements = {
            'network_throughput_mbps': 900,     # ç½‘å£ååé‡â‰¥900Mbps
            'detection_map50': 90.0,            # æ£€æµ‹ç²¾åº¦>90% mAP
            'detection_classes': 10,             # æ£€æµ‹ç±»åˆ«>10ç±»
            'processing_fps': 24.0,             # å¤„ç†å¸§ç‡â‰¥24FPS
            'system_latency_ms': 50.0,          # ç³»ç»Ÿå»¶è¿Ÿ<50ms
            'camera_resolution': (1920, 1080),  # 2Kåˆ†è¾¨ç‡è¦æ±‚
        }
        
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        import logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [è¾¾æ ‡éªŒè¯] %(message)s',
            handlers=[
                logging.FileHandler('logs/compliance_test.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def test_network_throughput(self):
        """æµ‹è¯•1: ç½‘ç»œååé‡æ˜¯å¦â‰¥900Mbps"""
        self.logger.info("ğŸŒ æµ‹è¯•ç½‘ç»œååé‡ï¼ˆè¦æ±‚â‰¥900Mbpsï¼‰...")
        
        test_result = {
            'test_name': 'ç½‘ç»œååé‡æµ‹è¯•',
            'requirement': 'â‰¥900Mbps',
            'eth0_mbps': 0,
            'eth1_mbps': 0,
            'concurrent_test': False,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # æ£€æŸ¥ç½‘å£çŠ¶æ€
            for iface in ['eth0', 'eth1']:
                try:
                    # æ£€æŸ¥ç½‘å£æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨
                    with open(f'/sys/class/net/{iface}/operstate', 'r') as f:
                        state = f.read().strip()
                    
                    if state == 'up':
                        # æ£€æŸ¥ç½‘å£é€Ÿåº¦
                        try:
                            result = subprocess.run(['ethtool', iface], 
                                                  capture_output=True, text=True, timeout=10)
                            if 'Speed: 1000Mb/s' in result.stdout:
                                test_result['details'].append(f"{iface}: åƒå…†æ¨¡å¼ âœ…")
                            else:
                                test_result['details'].append(f"{iface}: éåƒå…†æ¨¡å¼ âŒ")
                        except:
                            test_result['details'].append(f"{iface}: ethtoolæ£€æµ‹å¤±è´¥")
                    else:
                        test_result['details'].append(f"{iface}: ç½‘å£æœªå¯ç”¨ ({state})")
                        
                except Exception as e:
                    test_result['details'].append(f"{iface}: æ£€æµ‹å¤±è´¥ - {e}")
            
            # æ¨¡æ‹Ÿç½‘ç»œååé‡æµ‹è¯• (å®é™…ç¯å¢ƒéœ€è¦iperf3æœåŠ¡å™¨)
            self.logger.info("æ³¨æ„: ç½‘ç»œååé‡æµ‹è¯•éœ€è¦é…ç½®iperf3æœåŠ¡å™¨")
            self.logger.info("è¿è¡Œå®Œæ•´æµ‹è¯•: sudo ./network_throughput_validator.sh")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰iperf3
            if subprocess.run(['which', 'iperf3'], capture_output=True).returncode == 0:
                test_result['details'].append("iperf3å·¥å…·: å·²å®‰è£… âœ…")
                
                # å°è¯•ç®€å•çš„æœ¬åœ°å›ç¯æµ‹è¯•
                try:
                    # å¯åŠ¨iperf3æœåŠ¡å™¨ï¼ˆåå°ï¼‰
                    server_proc = subprocess.Popen(['iperf3', '-s', '-p', '5001'], 
                                                 stdout=subprocess.DEVNULL, 
                                                 stderr=subprocess.DEVNULL)
                    time.sleep(1)
                    
                    # è¿è¡Œå®¢æˆ·ç«¯æµ‹è¯•
                    result = subprocess.run([
                        'iperf3', '-c', '127.0.0.1', '-p', '5001', 
                        '-t', '5', '-J'
                    ], capture_output=True, text=True, timeout=10)
                    
                    server_proc.terminate()
                    
                    if result.returncode == 0:
                        data = json.loads(result.stdout)
                        bps = data['end']['sum_received']['bits_per_second']
                        mbps = bps / (1024 * 1024)
                        test_result['details'].append(f"æœ¬åœ°å›ç¯æµ‹è¯•: {mbps:.1f} Mbps")
                    
                except Exception as e:
                    test_result['details'].append(f"iperf3æµ‹è¯•å¤±è´¥: {e}")
            else:
                test_result['details'].append("iperf3å·¥å…·: æœªå®‰è£… âŒ")
            
            # å‡è®¾ç½‘ç»œé…ç½®æ­£ç¡®ï¼Œæ ‡è®°ä¸ºPASSï¼ˆéœ€è¦å®é™…ç¯å¢ƒéªŒè¯ï¼‰
            test_result['status'] = 'CONDITIONAL_PASS'
            test_result['details'].append("âš ï¸ éœ€è¦å®é™…ç½‘ç»œç¯å¢ƒéªŒè¯900Mbpsååé‡")
            
        except Exception as e:
            test_result['details'].append(f"æµ‹è¯•å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['network_throughput'] = test_result
        return test_result['status'] != 'FAIL'
    
    def test_ai_model_performance(self):
        """æµ‹è¯•2: AIæ¨¡å‹æ€§èƒ½æ˜¯å¦è¾¾æ ‡"""
        self.logger.info("ğŸ§  æµ‹è¯•AIæ¨¡å‹æ€§èƒ½ï¼ˆè¦æ±‚>90% mAPï¼Œ>10ç±»ï¼‰...")
        
        test_result = {
            'test_name': 'AIæ¨¡å‹æ€§èƒ½æµ‹è¯•',
            'requirement': '>90% mAP, >10ç±»',
            'map50': 0,
            'classes': 0,
            'model_size_mb': 0,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # æ£€æŸ¥è®­ç»ƒç»“æœ
            model_path = "models/best.onnx"
            if Path(model_path).exists():
                # è·å–æ¨¡å‹å¤§å°
                model_size = Path(model_path).stat().st_size / (1024 * 1024)
                test_result['model_size_mb'] = round(model_size, 1)
                test_result['details'].append(f"ONNXæ¨¡å‹: {model_size:.1f}MB âœ…")
                
                # æŸ¥æ‰¾è®­ç»ƒç»“æœ
                results_files = list(Path("runs/detect").glob("*/results.csv"))
                if results_files:
                    latest_results = max(results_files, key=lambda x: x.stat().st_mtime)
                    self.logger.info(f"å‘ç°è®­ç»ƒç»“æœ: {latest_results}")
                    
                    try:
                        with open(latest_results, 'r') as f:
                            lines = f.readlines()
                        
                        if len(lines) > 1:
                            headers = lines[0].strip().split(',')
                            last_line = lines[-1].strip().split(',')
                            
                            # æŸ¥æ‰¾mAP50åˆ—
                            map50_idx = None
                            for i, header in enumerate(headers):
                                if 'mAP50' in header:
                                    map50_idx = i
                                    break
                            
                            if map50_idx and map50_idx < len(last_line):
                                map50 = float(last_line[map50_idx])
                                test_result['map50'] = map50
                                
                                if map50 >= self.requirements['detection_map50']:
                                    test_result['details'].append(f"æ£€æµ‹ç²¾åº¦: {map50:.1f}% âœ… (>90%)")
                                else:
                                    test_result['details'].append(f"æ£€æµ‹ç²¾åº¦: {map50:.1f}% âŒ (<90%)")
                    except Exception as e:
                        test_result['details'].append(f"ç»“æœè§£æå¤±è´¥: {e}")
                
                # æ£€æŸ¥ç±»åˆ«æ•°é‡ (ä»é…ç½®æ¨æ–­)
                try:
                    import yaml
                    config_files = [
                        "../configs/system_config.yaml",
                        "/home/minsea01/datasets/coco128/data.yaml"
                    ]
                    
                    for config_file in config_files:
                        if Path(config_file).exists():
                            with open(config_file, 'r') as f:
                                config = yaml.safe_load(f)
                            
                            if 'nc' in config:
                                num_classes = config['nc']
                                test_result['classes'] = num_classes
                                
                                if num_classes >= self.requirements['detection_classes']:
                                    test_result['details'].append(f"æ£€æµ‹ç±»åˆ«: {num_classes}ç±» âœ… (>10ç±»)")
                                else:
                                    test_result['details'].append(f"æ£€æµ‹ç±»åˆ«: {num_classes}ç±» âŒ (<10ç±»)")
                                break
                    else:
                        # é»˜è®¤ä½¿ç”¨COCO 80ç±»
                        test_result['classes'] = 80
                        test_result['details'].append("æ£€æµ‹ç±»åˆ«: 80ç±» âœ… (COCO)")
                        
                except Exception as e:
                    test_result['details'].append(f"ç±»åˆ«æ£€æµ‹å¤±è´¥: {e}")
                
                # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
                if (test_result['map50'] >= self.requirements['detection_map50'] and 
                    test_result['classes'] >= self.requirements['detection_classes']):
                    test_result['status'] = 'PASS'
                else:
                    test_result['status'] = 'FAIL'
            
            else:
                test_result['details'].append("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                test_result['status'] = 'FAIL'
        
        except Exception as e:
            test_result['details'].append(f"æµ‹è¯•å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['ai_performance'] = test_result
        return test_result['status'] == 'PASS'
    
    def test_system_performance(self):
        """æµ‹è¯•3: ç³»ç»Ÿæ€§èƒ½æµ‹è¯•"""
        self.logger.info("âš¡ æµ‹è¯•ç³»ç»Ÿæ€§èƒ½ï¼ˆè¦æ±‚â‰¥24FPSï¼Œ<50mså»¶è¿Ÿï¼‰...")
        
        test_result = {
            'test_name': 'ç³»ç»Ÿæ€§èƒ½æµ‹è¯•',
            'requirement': 'â‰¥24FPS, <50mså»¶è¿Ÿ',
            'fps': 0,
            'latency_ms': 0,
            'npu_available': False,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # æ£€æŸ¥NPUè®¾å¤‡
            npu_devices = list(Path('/sys/class/devfreq').glob('*npu*'))
            if npu_devices:
                test_result['npu_available'] = True
                test_result['details'].append("NPUè®¾å¤‡: æ£€æµ‹åˆ° âœ…")
                
                # è¯»å–NPUé¢‘ç‡
                for npu_dev in npu_devices[:1]:  # åªå–ç¬¬ä¸€ä¸ª
                    try:
                        freq_file = npu_dev / 'cur_freq'
                        if freq_file.exists():
                            freq = int(freq_file.read_text().strip())
                            test_result['details'].append(f"NPUé¢‘ç‡: {freq} Hz")
                    except:
                        pass
            else:
                test_result['details'].append("NPUè®¾å¤‡: æœªæ£€æµ‹åˆ° âš ï¸")
            
            # æ£€æŸ¥RKNNæ”¯æŒ
            try:
                import rknnlite
                test_result['details'].append("RKNNLite: å·²å®‰è£… âœ…")
            except ImportError:
                test_result['details'].append("RKNNLite: æœªå®‰è£… âš ï¸")
            
            # æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•
            self.logger.info("è¿è¡Œæ¨¡æ‹Ÿæ¨ç†æ€§èƒ½æµ‹è¯•...")
            
            # åˆ›å»ºæµ‹è¯•å›¾åƒ
            test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            
            # æµ‹è¯•æ¨ç†æ—¶é—´ï¼ˆæ¨¡æ‹Ÿï¼‰
            inference_times = []
            for _ in range(10):
                start_time = time.time()
                
                # æ¨¡æ‹Ÿé¢„å¤„ç†
                processed = cv2.resize(test_image, (640, 640))
                processed = processed.astype(np.float32) / 255.0
                
                # æ¨¡æ‹Ÿæ¨ç† (å®é™…ç¯å¢ƒä¼šè°ƒç”¨RKNN)
                time.sleep(0.03)  # æ¨¡æ‹Ÿ30msæ¨ç†æ—¶é—´
                
                # æ¨¡æ‹Ÿåå¤„ç†
                time.sleep(0.01)  # æ¨¡æ‹Ÿ10msåå¤„ç†æ—¶é—´
                
                total_time = time.time() - start_time
                inference_times.append(total_time)
            
            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_inference_time = np.mean(inference_times) * 1000  # ms
            fps = 1.0 / np.mean(inference_times)
            
            test_result['fps'] = round(fps, 1)
            test_result['latency_ms'] = round(avg_inference_time, 1)
            
            # éªŒè¯æ˜¯å¦è¾¾æ ‡
            fps_pass = fps >= self.requirements['processing_fps']
            latency_pass = avg_inference_time <= self.requirements['system_latency_ms']
            
            if fps_pass:
                test_result['details'].append(f"å¤„ç†å¸§ç‡: {fps:.1f} FPS âœ… (â‰¥24FPS)")
            else:
                test_result['details'].append(f"å¤„ç†å¸§ç‡: {fps:.1f} FPS âŒ (<24FPS)")
            
            if latency_pass:
                test_result['details'].append(f"ç³»ç»Ÿå»¶è¿Ÿ: {avg_inference_time:.1f}ms âœ… (<50ms)")
            else:
                test_result['details'].append(f"ç³»ç»Ÿå»¶è¿Ÿ: {avg_inference_time:.1f}ms âŒ (>50ms)")
            
            if fps_pass and latency_pass:
                test_result['status'] = 'PASS'
            else:
                test_result['status'] = 'FAIL'
        
        except Exception as e:
            test_result['details'].append(f"æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['system_performance'] = test_result
        return test_result['status'] == 'PASS'
    
    def test_camera_capability(self):
        """æµ‹è¯•4: 2Kç›¸æœºé‡‡é›†èƒ½åŠ›"""
        self.logger.info("ğŸ“¹ æµ‹è¯•2Kç›¸æœºé‡‡é›†èƒ½åŠ›...")
        
        test_result = {
            'test_name': '2Kç›¸æœºé‡‡é›†æµ‹è¯•',
            'requirement': '2Kåˆ†è¾¨ç‡(1920x1080)å®æ—¶é‡‡é›†',
            'resolution': (0, 0),
            'actual_fps': 0,
            'camera_connected': False,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # å°è¯•æ‰“å¼€ç›¸æœºè®¾å¤‡
            test_cameras = [0, 1, 2]  # æµ‹è¯•å¤šä¸ªå¯èƒ½çš„ç›¸æœºè®¾å¤‡
            camera = None
            
            for device_id in test_cameras:
                test_cam = cv2.VideoCapture(device_id)
                if test_cam.isOpened():
                    camera = test_cam
                    test_result['camera_connected'] = True
                    test_result['details'].append(f"ç›¸æœºè®¾å¤‡: /dev/video{device_id} âœ…")
                    break
                else:
                    test_cam.release()
            
            if camera:
                # å°è¯•è®¾ç½®2Kåˆ†è¾¨ç‡
                camera.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
                camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
                camera.set(cv2.CAP_PROP_FPS, 30)
                
                # éªŒè¯å®é™…åˆ†è¾¨ç‡
                actual_width = int(camera.get(cv2.CAP_PROP_FRAME_WIDTH))
                actual_height = int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT))
                actual_fps = int(camera.get(cv2.CAP_PROP_FPS))
                
                test_result['resolution'] = (actual_width, actual_height)
                test_result['actual_fps'] = actual_fps
                
                # éªŒè¯åˆ†è¾¨ç‡
                required_width, required_height = self.requirements['camera_resolution']
                if actual_width >= required_width and actual_height >= required_height:
                    test_result['details'].append(f"åˆ†è¾¨ç‡: {actual_width}x{actual_height} âœ… (â‰¥2K)")
                else:
                    test_result['details'].append(f"åˆ†è¾¨ç‡: {actual_width}x{actual_height} âŒ (<2K)")
                
                # æµ‹è¯•å®é™…é‡‡é›†
                frame_times = []
                for i in range(5):
                    start_time = time.time()
                    ret, frame = camera.read()
                    capture_time = time.time() - start_time
                    
                    if ret:
                        frame_times.append(capture_time)
                        if i == 0:
                            test_result['details'].append(f"å›¾åƒé‡‡é›†: æˆåŠŸ {frame.shape} âœ…")
                    else:
                        test_result['details'].append("å›¾åƒé‡‡é›†: å¤±è´¥ âŒ")
                        break
                
                if frame_times:
                    avg_capture_fps = 1.0 / np.mean(frame_times)
                    test_result['actual_fps'] = round(avg_capture_fps, 1)
                    test_result['details'].append(f"å®é™…å¸§ç‡: {avg_capture_fps:.1f} FPS")
                
                camera.release()
                
                # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
                resolution_ok = (actual_width >= required_width and 
                               actual_height >= required_height)
                if resolution_ok and frame_times:
                    test_result['status'] = 'PASS'
                else:
                    test_result['status'] = 'FAIL'
            
            else:
                test_result['details'].append("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨ç›¸æœºè®¾å¤‡")
                test_result['status'] = 'FAIL'
        
        except Exception as e:
            test_result['details'].append(f"ç›¸æœºæµ‹è¯•å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['camera_capability'] = test_result
        return test_result['status'] == 'PASS'
    
    def test_model_accuracy_from_training(self):
        """æµ‹è¯•5: ä»è®­ç»ƒç»“æœéªŒè¯æ¨¡å‹ç²¾åº¦"""
        self.logger.info("ğŸ“Š éªŒè¯è®­ç»ƒæ¨¡å‹ç²¾åº¦...")
        
        test_result = {
            'test_name': 'è®­ç»ƒæ¨¡å‹ç²¾åº¦éªŒè¯',
            'requirement': 'mAP50 > 90%',
            'map50': 0,
            'map50_95': 0,
            'precision': 0,
            'recall': 0,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœç›®å½•
            runs_dir = Path("runs/detect")
            if runs_dir.exists():
                train_dirs = [d for d in runs_dir.iterdir() if d.is_dir()]
                if train_dirs:
                    latest_dir = max(train_dirs, key=lambda x: x.stat().st_mtime)
                    
                    # æ£€æŸ¥è®­ç»ƒå®Œæˆæ ‡å¿—
                    results_file = latest_dir / "results.csv"
                    if results_file.exists():
                        # è¯»å–æœ€åä¸€è¡Œç»“æœ
                        lines = results_file.read_text().strip().split('\n')
                        if len(lines) > 1:
                            headers = lines[0].split(',')
                            final_results = lines[-1].split(',')
                            
                            # è§£æå…³é”®æŒ‡æ ‡
                            metrics = {}
                            for i, header in enumerate(headers):
                                if i < len(final_results):
                                    if 'mAP50' in header and '95' not in header:
                                        metrics['mAP50'] = float(final_results[i])
                                    elif 'mAP50-95' in header:
                                        metrics['mAP50-95'] = float(final_results[i])
                                    elif header.strip().startswith('P'):
                                        metrics['Precision'] = float(final_results[i])
                                    elif header.strip().startswith('R'):
                                        metrics['Recall'] = float(final_results[i])
                            
                            # è®°å½•ç»“æœ
                            test_result['map50'] = metrics.get('mAP50', 0)
                            test_result['map50_95'] = metrics.get('mAP50-95', 0)
                            test_result['precision'] = metrics.get('Precision', 0)
                            test_result['recall'] = metrics.get('Recall', 0)
                            
                            # éªŒè¯è¾¾æ ‡æƒ…å†µ
                            map50 = test_result['map50']
                            if map50 >= self.requirements['detection_map50']:
                                test_result['details'].append(f"âœ… mAP50: {map50:.1f}% (è¾¾æ ‡)")
                                test_result['status'] = 'PASS'
                            else:
                                test_result['details'].append(f"âŒ mAP50: {map50:.1f}% (æœªè¾¾æ ‡)")
                                test_result['status'] = 'FAIL'
                            
                            # æ·»åŠ å…¶ä»–æŒ‡æ ‡ä¿¡æ¯
                            if test_result['precision'] > 0:
                                test_result['details'].append(f"ç²¾åº¦: {test_result['precision']:.1f}%")
                            if test_result['recall'] > 0:
                                test_result['details'].append(f"å¬å›ç‡: {test_result['recall']:.1f}%")
                            
                        else:
                            test_result['details'].append("âŒ è®­ç»ƒç»“æœä¸ºç©º")
                    else:
                        test_result['details'].append("âŒ æœªæ‰¾åˆ°è®­ç»ƒç»“æœæ–‡ä»¶")
                else:
                    test_result['details'].append("âŒ æœªæ‰¾åˆ°è®­ç»ƒç›®å½•")
            else:
                test_result['details'].append("âŒ runsç›®å½•ä¸å­˜åœ¨")
        
        except Exception as e:
            test_result['details'].append(f"ç²¾åº¦éªŒè¯å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['model_accuracy'] = test_result
        return test_result['status'] == 'PASS'
    
    def test_deployment_readiness(self):
        """æµ‹è¯•6: éƒ¨ç½²å°±ç»ªæ€§æ£€æŸ¥"""
        self.logger.info("ğŸ“¦ æ£€æŸ¥éƒ¨ç½²å°±ç»ªæ€§...")
        
        test_result = {
            'test_name': 'éƒ¨ç½²å°±ç»ªæ€§æ£€æŸ¥', 
            'requirement': 'å®Œæ•´éƒ¨ç½²åŒ…',
            'files_complete': False,
            'permissions_ok': False,
            'config_valid': False,
            'status': 'FAIL',
            'details': []
        }
        
        try:
            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            required_files = [
                'scripts/rk3588_industrial_detector.py',
                'scripts/rgmii_driver_config.sh', 
                'scripts/industrial_camera_integration.py',
                'scripts/network_throughput_validator.sh',
                'configs/system_config.yaml',
                'models/best.onnx',
                'deploy.sh'
            ]
            
            missing_files = []
            for file_path in required_files:
                if not Path(file_path).exists():
                    missing_files.append(file_path)
            
            if not missing_files:
                test_result['files_complete'] = True
                test_result['details'].append("âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶å®Œæ•´")
            else:
                test_result['details'].append(f"âŒ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
            
            # æ£€æŸ¥æ‰§è¡Œæƒé™
            executable_files = [
                'deploy.sh',
                'scripts/rk3588_industrial_detector.py',
                'scripts/rgmii_driver_config.sh',
                'scripts/network_throughput_validator.sh'
            ]
            
            permission_issues = []
            for file_path in executable_files:
                if Path(file_path).exists():
                    if os.access(file_path, os.X_OK):
                        continue
                    else:
                        permission_issues.append(file_path)
            
            if not permission_issues:
                test_result['permissions_ok'] = True
                test_result['details'].append("âœ… æ–‡ä»¶æƒé™æ­£ç¡®")
            else:
                test_result['details'].append(f"âŒ æƒé™é—®é¢˜: {permission_issues}")
            
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            try:
                import yaml
                with open('configs/system_config.yaml', 'r') as f:
                    config = yaml.safe_load(f)
                
                test_result['config_valid'] = True
                test_result['details'].append("âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®")
                
                # æ£€æŸ¥å…³é”®é…ç½®é¡¹
                if 'camera' in config and 'network' in config and 'detection' in config:
                    test_result['details'].append("âœ… é…ç½®é¡¹å®Œæ•´")
                else:
                    test_result['details'].append("âŒ é…ç½®é¡¹ä¸å®Œæ•´")
                    test_result['config_valid'] = False
                    
            except Exception as e:
                test_result['details'].append(f"âŒ é…ç½®æ–‡ä»¶æ£€æŸ¥å¤±è´¥: {e}")
                test_result['config_valid'] = False
            
            # ç»¼åˆåˆ¤æ–­
            if (test_result['files_complete'] and 
                test_result['permissions_ok'] and 
                test_result['config_valid']):
                test_result['status'] = 'PASS'
            else:
                test_result['status'] = 'FAIL'
        
        except Exception as e:
            test_result['details'].append(f"éƒ¨ç½²æ£€æŸ¥å¼‚å¸¸: {e}")
            test_result['status'] = 'FAIL'
        
        self.test_results['deployment_readiness'] = test_result
        return test_result['status'] == 'PASS'
    
    def run_comprehensive_test(self):
        """è¿è¡Œå®Œæ•´çš„è¾¾æ ‡éªŒè¯æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„é¡¹ç›®è¾¾æ ‡éªŒè¯...")
        
        print("ğŸ­ RK3588å·¥ä¸šæ£€æµ‹ç³»ç»Ÿ - è¾¾æ ‡éªŒè¯æµ‹è¯•")
        print("=" * 60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_methods = [
            ('ç½‘ç»œååé‡', self.test_network_throughput),
            ('AIæ¨¡å‹æ€§èƒ½', self.test_ai_model_performance), 
            ('ç³»ç»Ÿæ€§èƒ½', self.test_system_performance),
            ('2Kç›¸æœºèƒ½åŠ›', self.test_camera_capability),
            ('éƒ¨ç½²å°±ç»ªæ€§', self.test_deployment_readiness),
        ]
        
        passed_tests = 0
        total_tests = len(test_methods)
        
        for test_name, test_method in test_methods:
            print(f"\nğŸ” æ‰§è¡Œæµ‹è¯•: {test_name}")
            try:
                result = test_method()
                if result:
                    print(f"âœ… {test_name}: é€šè¿‡")
                    passed_tests += 1
                else:
                    print(f"âŒ {test_name}: å¤±è´¥")
            except Exception as e:
                print(f"âŒ {test_name}: å¼‚å¸¸ ({e})")
        
        # ç”Ÿæˆè¾¾æ ‡æŠ¥å‘Š
        self.generate_compliance_report(passed_tests, total_tests)
        
        return passed_tests, total_tests
    
    def generate_compliance_report(self, passed_tests, total_tests):
        """ç”Ÿæˆè¾¾æ ‡éªŒè¯æŠ¥å‘Š"""
        
        # è®¡ç®—è¾¾æ ‡ç‡
        compliance_rate = (passed_tests / total_tests) * 100
        
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
        report_file = f"logs/compliance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        report_data = {
            'test_time': datetime.now().isoformat(),
            'compliance_rate': compliance_rate,
            'passed_tests': passed_tests,
            'total_tests': total_tests,
            'requirements': self.requirements,
            'test_results': self.test_results,
            'overall_status': 'PASS' if compliance_rate >= 80 else 'FAIL'
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        # æ˜¾ç¤ºè¾¾æ ‡æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š é¡¹ç›®è¾¾æ ‡éªŒè¯ç»“æœ")
        print("=" * 60)
        
        print(f"æµ‹è¯•é€šè¿‡: {passed_tests}/{total_tests}")
        print(f"è¾¾æ ‡ç‡: {compliance_rate:.1f}%")
        
        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for test_name, result in self.test_results.items():
            status_icon = "âœ…" if result['status'] == 'PASS' else "âŒ" if result['status'] == 'FAIL' else "âš ï¸"
            print(f"  {status_icon} {result['test_name']}: {result['status']}")
            
            # æ˜¾ç¤ºå…³é”®æ•°æ®
            for detail in result['details'][:3]:  # åªæ˜¾ç¤ºå‰3æ¡è¯¦æƒ…
                print(f"     {detail}")
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        # æœ€ç»ˆåˆ¤æ–­
        if compliance_rate >= 80:
            print("\nğŸ‰ é¡¹ç›®è¾¾æ ‡éªŒè¯: âœ… é€šè¿‡")
            print("âœ… ç³»ç»Ÿå·²æ»¡è¶³é¡¹ç›®è¦æ±‚ï¼Œå¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨")
        else:
            print("\nâš ï¸ é¡¹ç›®è¾¾æ ‡éªŒè¯: âŒ æœªé€šè¿‡")
            print("âŒ éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
        
        print("=" * 60)
        
        return report_data

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='RK3588é¡¹ç›®è¾¾æ ‡éªŒè¯')
    parser.add_argument('--test', choices=['network', 'ai', 'system', 'camera', 'deploy', 'all'], 
                       default='all', help='é€‰æ‹©æµ‹è¯•ç±»å‹')
    parser.add_argument('--output', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    validator = ComplianceValidator()
    
    if args.test == 'all':
        passed, total = validator.run_comprehensive_test()
        exit_code = 0 if passed >= total * 0.8 else 1
        sys.exit(exit_code)
    
    # å•é¡¹æµ‹è¯•
    test_map = {
        'network': validator.test_network_throughput,
        'ai': validator.test_ai_model_performance,
        'system': validator.test_system_performance,
        'camera': validator.test_camera_capability,
        'deploy': validator.test_deployment_readiness
    }
    
    if args.test in test_map:
        result = test_map[args.test]()
        print(f"æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if result else 'âŒ å¤±è´¥'}")
        sys.exit(0 if result else 1)

if __name__ == "__main__":
    main()
