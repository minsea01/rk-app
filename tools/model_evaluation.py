#!/usr/bin/env python3
"""
YOLOæ¨¡å‹è¯„ä¼°è„šæœ¬
ç”ŸæˆPRæ›²çº¿ã€æ··æ·†çŸ©é˜µã€é¢„æµ‹æ ·ä¾‹ç­‰è¯Šæ–­å›¾è¡¨

ç”¨æ³•:
    python tools/model_evaluation.py --model runs/train/exp/weights/best.pt --data /path/to/data.yaml
    python tools/model_evaluation.py --model runs/train/industrial_15cls_test5/weights/best.pt --data industrial_dataset/data.yaml

è¾“å‡º:
    1. è¯¦ç»†è¯„ä¼°æŠ¥å‘Š (evaluation_report.txt)
    2. PRæ›²çº¿å›¾ (pr_curves.png)
    3. æ··æ·†çŸ©é˜µ (confusion_matrix.png)
    4. é¢„æµ‹æ ·ä¾‹å¯¹æ¯” (prediction_samples.png)
    5. ç½®ä¿¡åº¦åˆ†å¸ƒ (confidence_distribution.png)
"""

import os
import sys
import yaml
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import numpy as np
import json
from collections import defaultdict
import seaborn as sns
from ultralytics import YOLO
import torch

class ModelEvaluator:
    def __init__(self, model_path, data_yaml_path, conf_threshold=0.25, iou_threshold=0.6):
        self.model_path = Path(model_path)
        self.data_yaml_path = Path(data_yaml_path)
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        
        self.load_model()
        self.load_config()
        
    def load_model(self):
        """åŠ è½½YOLOæ¨¡å‹"""
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.model_path}")
        self.model = YOLO(str(self.model_path))
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    def load_config(self):
        """åŠ è½½æ•°æ®é›†é…ç½®"""
        with open(self.data_yaml_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        self.dataset_path = Path(self.config['path'])
        self.num_classes = self.config['nc']
        self.class_names = self.config.get('names', [f'class_{i}' for i in range(self.num_classes)])
        
        print(f"ğŸ“Š æ•°æ®é›†: {self.dataset_path}")
        print(f"ğŸ“Š ç±»åˆ«æ•°: {self.num_classes}")
        print(f"ğŸ“Š ç±»åˆ«: {self.class_names}")
        
    def run_validation(self, split='val'):
        """è¿è¡ŒéªŒè¯å¹¶è·å–è¯¦ç»†ç»“æœ"""
        print(f"\nğŸ”¬ è¿è¡Œ {split} é›†éªŒè¯...")
        
        # ä½¿ç”¨ultralyticså†…ç½®éªŒè¯
        results = self.model.val(
            data=str(self.data_yaml_path),
            split=split,
            conf=self.conf_threshold,
            iou=self.iou_threshold,
            plots=False,  # æˆ‘ä»¬è‡ªå·±ç”Ÿæˆå›¾è¡¨
            save_json=True,
            device='0' if torch.cuda.is_available() else 'cpu'
        )
        
        self.val_results = results
        return results
    
    def analyze_predictions(self, split='val', max_samples=100):
        """åˆ†æé¢„æµ‹ç»“æœ"""
        print(f"\nğŸ” åˆ†æ {split} é›†é¢„æµ‹ç»“æœ...")
        
        img_dir = self.dataset_path / self.config[split]
        
        # è·å–å›¾åƒæ–‡ä»¶åˆ—è¡¨
        img_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            img_files.extend(list(img_dir.glob(ext)))
            
        if len(img_files) > max_samples:
            img_files = img_files[:max_samples]
            
        predictions = []
        confidence_scores = []
        
        for img_file in img_files:
            # é¢„æµ‹
            results = self.model(str(img_file), conf=self.conf_threshold, verbose=False)
            
            for result in results:
                if result.boxes is not None:
                    boxes = result.boxes.data.cpu().numpy()
                    for box in boxes:
                        x1, y1, x2, y2, conf, cls = box
                        confidence_scores.append(conf)
                        predictions.append({
                            'image': str(img_file),
                            'bbox': [x1, y1, x2, y2],
                            'confidence': conf,
                            'class': int(cls),
                            'class_name': self.class_names[int(cls)]
                        })
        
        self.predictions = predictions
        self.confidence_scores = confidence_scores
        
        print(f"ğŸ“Š æ€»é¢„æµ‹æ•°: {len(predictions)}")
        print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦èŒƒå›´: {np.min(confidence_scores):.3f} - {np.max(confidence_scores):.3f}")
        
        return predictions, confidence_scores
    
    def plot_pr_curves(self, output_path='pr_curves.png'):
        """ç»˜åˆ¶PRæ›²çº¿"""
        print(f"\nğŸ“ˆ ç”ŸæˆPRæ›²çº¿...")
        
        if not hasattr(self.val_results, 'curves'):
            print("âš ï¸ æ— æ³•è·å–PRæ›²çº¿æ•°æ®")
            return
            
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # æ•´ä½“PRæ›²çº¿
        if hasattr(self.val_results, 'pr_curve'):
            axes[0, 0].plot(self.val_results.pr_curve[0], self.val_results.pr_curve[1])
            axes[0, 0].set_xlabel('Recall')
            axes[0, 0].set_ylabel('Precision')
            axes[0, 0].set_title('Overall PR Curve')
            axes[0, 0].grid(True, alpha=0.3)
            
        # å„ç±»åˆ«PRæ›²çº¿ï¼ˆå¦‚æœæœ‰è¯¦ç»†æ•°æ®ï¼‰
        axes[0, 1].text(0.5, 0.5, 'Per-Class PR Curves\n(éœ€è¦è¯¦ç»†éªŒè¯æ•°æ®)', 
                       ha='center', va='center', transform=axes[0, 1].transAxes)
        axes[0, 1].set_title('Per-Class PR Curves')
        
        # mAP@0.5æ›²çº¿
        if hasattr(self.val_results, 'maps'):
            maps = self.val_results.maps
            axes[1, 0].bar(range(len(maps)), maps, color='skyblue')
            axes[1, 0].set_xlabel('Class')
            axes[1, 0].set_ylabel('mAP@0.5')
            axes[1, 0].set_title('mAP@0.5 per Class')
            axes[1, 0].set_xticks(range(len(self.class_names)))
            axes[1, 0].set_xticklabels(self.class_names, rotation=45, ha='right')
            
        # å…³é”®æŒ‡æ ‡æ±‡æ€»
        metrics_text = f"""
å…³é”®æŒ‡æ ‡:
mAP@0.5: {self.val_results.box.map50:.3f}
mAP@0.5:0.95: {self.val_results.box.map:.3f}
Precision: {self.val_results.box.mp:.3f}
Recall: {self.val_results.box.mr:.3f}
        """
        axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes, 
                        fontsize=12, verticalalignment='center', fontfamily='monospace')
        axes[1, 1].set_title('Key Metrics Summary')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“ˆ PRæ›²çº¿å·²ä¿å­˜: {output_path}")
    
    def plot_confusion_matrix(self, output_path='confusion_matrix.png'):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        print(f"\nğŸ”„ ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        
        if hasattr(self.val_results, 'confusion_matrix') and self.val_results.confusion_matrix is not None:
            cm = self.val_results.confusion_matrix.matrix
            
            plt.figure(figsize=(12, 10))
            
            # å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
            cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            cm_norm = np.nan_to_num(cm_norm)
            
            # ç»˜åˆ¶çƒ­åŠ›å›¾
            sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',
                       xticklabels=self.class_names + ['Background'],
                       yticklabels=self.class_names + ['Background'])
            
            plt.title('Normalized Confusion Matrix')
            plt.xlabel('Predicted')
            plt.ylabel('True')
            plt.xticks(rotation=45, ha='right')
            plt.yticks(rotation=0)
            
            plt.tight_layout()
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            plt.close()
            print(f"ğŸ”„ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {output_path}")
        else:
            print("âš ï¸ æ— æ³•è·å–æ··æ·†çŸ©é˜µæ•°æ®")
    
    def plot_confidence_distribution(self, output_path='confidence_distribution.png'):
        """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ"""
        print(f"\nğŸ“Š ç”Ÿæˆç½®ä¿¡åº¦åˆ†å¸ƒ...")
        
        if not hasattr(self, 'confidence_scores'):
            print("âš ï¸ è¯·å…ˆè¿è¡Œanalyze_predictions")
            return
            
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ•´ä½“ç½®ä¿¡åº¦åˆ†å¸ƒ
        ax1.hist(self.confidence_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.axvline(np.mean(self.confidence_scores), color='red', linestyle='--', 
                   label=f'Mean: {np.mean(self.confidence_scores):.3f}')
        ax1.axvline(self.conf_threshold, color='orange', linestyle='--', 
                   label=f'Threshold: {self.conf_threshold}')
        ax1.set_xlabel('Confidence Score')
        ax1.set_ylabel('Frequency')
        ax1.set_title('Confidence Score Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å„ç±»åˆ«ç½®ä¿¡åº¦åˆ†å¸ƒ
        class_confidences = defaultdict(list)
        for pred in self.predictions:
            class_confidences[pred['class_name']].append(pred['confidence'])
            
        # ç»˜åˆ¶ç®±çº¿å›¾
        if class_confidences:
            class_names = list(class_confidences.keys())
            conf_data = [class_confidences[name] for name in class_names]
            
            ax2.boxplot(conf_data, labels=class_names)
            ax2.set_xlabel('Class')
            ax2.set_ylabel('Confidence Score')
            ax2.set_title('Confidence Distribution by Class')
            ax2.tick_params(axis='x', rotation=45)
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š ç½®ä¿¡åº¦åˆ†å¸ƒå·²ä¿å­˜: {output_path}")
    
    def visualize_prediction_samples(self, split='val', num_samples=8, 
                                   output_path='prediction_samples.png'):
        """å¯è§†åŒ–é¢„æµ‹æ ·ä¾‹"""
        print(f"\nğŸ–¼ï¸ ç”Ÿæˆé¢„æµ‹æ ·ä¾‹å¯è§†åŒ–...")
        
        img_dir = self.dataset_path / self.config[split]
        label_dir = self.dataset_path / self.config[split].replace('/images', '/labels')
        if not label_dir.exists():
            label_dir = self.dataset_path / f'{split}/labels'
            
        # éšæœºé€‰æ‹©å›¾åƒ
        img_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
            img_files.extend(list(img_dir.glob(ext)))
            
        if len(img_files) < num_samples:
            num_samples = len(img_files)
            
        selected_files = np.random.choice(img_files, num_samples, replace=False)
        
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        axes = axes.ravel()
        
        colors = plt.cm.Set3(np.linspace(0, 1, self.num_classes))
        
        for i, img_file in enumerate(selected_files):
            if i >= num_samples:
                break
                
            # è¯»å–å›¾åƒ
            img = cv2.imread(str(img_file))
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            h, w = img.shape[:2]
            
            axes[i].imshow(img)
            axes[i].set_title(f"{img_file.name}")
            axes[i].axis('off')
            
            # ç»˜åˆ¶GTæ ‡æ³¨ï¼ˆç»¿è‰²ï¼‰
            label_file = label_dir / (img_file.stem + '.txt')
            if label_file.exists() and label_file.stat().st_size > 0:
                with open(label_file, 'r') as f:
                    for line in f:
                        parts = line.strip().split()
                        if len(parts) >= 5:
                            try:
                                class_id = int(parts[0])
                                x_center, y_center, width, height = map(float, parts[1:5])
                                
                                x = (x_center - width/2) * w
                                y = (y_center - height/2) * h
                                w_box = width * w
                                h_box = height * h
                                
                                rect = patches.Rectangle((x, y), w_box, h_box, 
                                                       linewidth=2, edgecolor='green', 
                                                       facecolor='none', linestyle='-')
                                axes[i].add_patch(rect)
                                
                                axes[i].text(x, y-15, f'GT: {self.class_names[class_id]}', 
                                           color='green', fontsize=8, 
                                           bbox=dict(boxstyle="round,pad=0.3", 
                                                   facecolor='white', alpha=0.7))
                            except (ValueError, IndexError):
                                continue
            
            # ç»˜åˆ¶é¢„æµ‹ç»“æœï¼ˆçº¢è‰²ï¼‰
            results = self.model(str(img_file), conf=self.conf_threshold, verbose=False)
            for result in results:
                if result.boxes is not None:
                    boxes = result.boxes.data.cpu().numpy()
                    for box in boxes:
                        x1, y1, x2, y2, conf, cls = box
                        class_id = int(cls)
                        
                        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, 
                                               linewidth=2, edgecolor='red', 
                                               facecolor='none', linestyle='--')
                        axes[i].add_patch(rect)
                        
                        axes[i].text(x1, y1-5, f'Pred: {self.class_names[class_id]} ({conf:.2f})', 
                                   color='red', fontsize=8, 
                                   bbox=dict(boxstyle="round,pad=0.3", 
                                           facecolor='white', alpha=0.7))
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.lines import Line2D
        legend_elements = [
            Line2D([0], [0], color='green', lw=2, label='Ground Truth'),
            Line2D([0], [0], color='red', lw=2, linestyle='--', label='Prediction')
        ]
        fig.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"ğŸ–¼ï¸ é¢„æµ‹æ ·ä¾‹å·²ä¿å­˜: {output_path}")
    
    def analyze_failure_cases(self):
        """åˆ†æå¤±æ•ˆæ¡ˆä¾‹"""
        print(f"\nğŸ” åˆ†æå¤±æ•ˆæ¡ˆä¾‹...")
        
        # ç®€åŒ–çš„å¤±æ•ˆåˆ†æ
        low_conf_predictions = [p for p in self.predictions if p['confidence'] < 0.5]
        high_conf_predictions = [p for p in self.predictions if p['confidence'] > 0.8]
        
        # ç±»åˆ«åˆ†å¸ƒåˆ†æ
        class_counts = defaultdict(int)
        low_conf_class_counts = defaultdict(int)
        
        for pred in self.predictions:
            class_counts[pred['class_name']] += 1
            
        for pred in low_conf_predictions:
            low_conf_class_counts[pred['class_name']] += 1
            
        failure_analysis = {
            'total_predictions': len(self.predictions),
            'low_confidence_count': len(low_conf_predictions),
            'high_confidence_count': len(high_conf_predictions),
            'low_confidence_ratio': len(low_conf_predictions) / len(self.predictions) if self.predictions else 0,
            'class_performance': {}
        }
        
        for class_name in class_counts:
            total = class_counts[class_name]
            low_conf = low_conf_class_counts.get(class_name, 0)
            failure_analysis['class_performance'][class_name] = {
                'total': total,
                'low_confidence': low_conf,
                'failure_rate': low_conf / total if total > 0 else 0
            }
        
        self.failure_analysis = failure_analysis
        return failure_analysis
    
    def generate_evaluation_report(self, output_path='evaluation_report.txt'):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print(f"\nğŸ“ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("YOLOæ¨¡å‹è¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
            f.write(f"æ•°æ®é›†: {self.dataset_path}\n")
            f.write(f"ç½®ä¿¡åº¦é˜ˆå€¼: {self.conf_threshold}\n")
            f.write(f"IoUé˜ˆå€¼: {self.iou_threshold}\n\n")
            
            # å…³é”®æŒ‡æ ‡
            f.write("ğŸ“Š å…³é”®æŒ‡æ ‡:\n")
            f.write("-" * 40 + "\n")
            f.write(f"mAP@0.5: {self.val_results.box.map50:.3f}\n")
            f.write(f"mAP@0.5:0.95: {self.val_results.box.map:.3f}\n")
            f.write(f"Precision: {self.val_results.box.mp:.3f}\n")
            f.write(f"Recall: {self.val_results.box.mr:.3f}\n")
            f.write(f"F1-Score: {2 * self.val_results.box.mp * self.val_results.box.mr / (self.val_results.box.mp + self.val_results.box.mr):.3f}\n\n")
            
            # é¢„æµ‹ç»Ÿè®¡
            if hasattr(self, 'predictions'):
                f.write("ğŸ” é¢„æµ‹ç»Ÿè®¡:\n")
                f.write("-" * 40 + "\n")
                f.write(f"æ€»é¢„æµ‹æ•°: {len(self.predictions)}\n")
                f.write(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(self.confidence_scores):.3f}\n")
                f.write(f"ç½®ä¿¡åº¦æ ‡å‡†å·®: {np.std(self.confidence_scores):.3f}\n")
                f.write(f"æœ€ä½ç½®ä¿¡åº¦: {np.min(self.confidence_scores):.3f}\n")
                f.write(f"æœ€é«˜ç½®ä¿¡åº¦: {np.max(self.confidence_scores):.3f}\n\n")
            
            # å¤±æ•ˆåˆ†æ
            if hasattr(self, 'failure_analysis'):
                fa = self.failure_analysis
                f.write("âš ï¸ å¤±æ•ˆåˆ†æ:\n")
                f.write("-" * 40 + "\n")
                f.write(f"ä½ç½®ä¿¡åº¦é¢„æµ‹æ¯”ä¾‹: {fa['low_confidence_ratio']:.3f}\n")
                f.write(f"é«˜ç½®ä¿¡åº¦é¢„æµ‹æ•°: {fa['high_confidence_count']}\n")
                f.write(f"ä½ç½®ä¿¡åº¦é¢„æµ‹æ•°: {fa['low_confidence_count']}\n\n")
                
                f.write("å„ç±»åˆ«å¤±æ•ˆç‡:\n")
                for class_name, perf in fa['class_performance'].items():
                    f.write(f"  {class_name}: {perf['failure_rate']:.3f} ({perf['low_confidence']}/{perf['total']})\n")
                f.write("\n")
            
            # è¯Šæ–­ç»“è®º
            f.write("ğŸ¯ è¯Šæ–­ç»“è®º:\n")
            f.write("-" * 40 + "\n")
            
            # åŸºäºæŒ‡æ ‡åˆ¤æ–­é—®é¢˜ç±»å‹
            precision = self.val_results.box.mp
            recall = self.val_results.box.mr
            
            if recall > 0.85 and precision < 0.65:
                f.write("âŒ å…¸å‹çš„'é«˜å¬å›ä½ç²¾åº¦'é—®é¢˜\n")
                f.write("å¯èƒ½åŸå› :\n")
                f.write("  â€¢ æ ‡ç­¾è´¨é‡é—®é¢˜ï¼ˆæ¼æ ‡ã€é”™æ ‡ï¼‰\n")
                f.write("  â€¢ ç›®æ ‡è¿‡å°æˆ–å¯†é›†ï¼Œéš¾ä»¥åŒºåˆ†\n")
                f.write("  â€¢ ç½®ä¿¡åº¦é˜ˆå€¼è¿‡ä½\n")
                f.write("  â€¢ ç±»åˆ«ä¸å‡è¡¡ä¸¥é‡\n\n")
                
                f.write("å»ºè®®æªæ–½:\n")
                f.write("  â€¢ è¿è¡Œæ•°æ®å¥åº·æ£€æŸ¥è„šæœ¬éªŒè¯æ ‡ç­¾è´¨é‡\n")
                f.write("  â€¢ æé«˜è¾“å…¥åˆ†è¾¨ç‡ (imgsz=960)\n")
                f.write("  â€¢ ä½¿ç”¨focal losså¤„ç†ç±»åˆ«ä¸å‡è¡¡\n")
                f.write("  â€¢ éƒ¨ç½²æ—¶æé«˜ç½®ä¿¡åº¦é˜ˆå€¼è‡³0.4-0.5\n")
                f.write("  â€¢ è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„æ•°æ®å¢å¼º\n")
                
            elif precision > 0.8 and recall < 0.6:
                f.write("âŒ 'é«˜ç²¾åº¦ä½å¬å›'é—®é¢˜\n")
                f.write("å¯èƒ½åŸå› :\n")
                f.write("  â€¢ æ¨¡å‹è¿‡äºä¿å®ˆï¼Œæ¼æ£€ä¸¥é‡\n")
                f.write("  â€¢ æ•°æ®å¢å¼ºè¿‡å¼º\n")
                f.write("  â€¢ å­¦ä¹ ç‡è¿‡å°æˆ–è®­ç»ƒä¸å……åˆ†\n\n")
                
                f.write("å»ºè®®æªæ–½:\n")
                f.write("  â€¢ é™ä½ç½®ä¿¡åº¦é˜ˆå€¼\n")
                f.write("  â€¢ å¢åŠ è®­ç»ƒè½®æ•°\n")
                f.write("  â€¢ è°ƒæ•´æŸå¤±æƒé‡ï¼Œå¢åŠ recallæƒé‡\n")
                
            elif precision > 0.7 and recall > 0.7:
                f.write("âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½\n")
                f.write("å¯ä»¥è€ƒè™‘:\n")
                f.write("  â€¢ è¿›ä¸€æ­¥ä¼˜åŒ–è¶…å‚æ•°\n")
                f.write("  â€¢ é’ˆå¯¹ç‰¹å®šåœºæ™¯å¾®è°ƒ\n")
                f.write("  â€¢ æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿ\n")
                
            else:
                f.write("âŒ æ•´ä½“æ€§èƒ½åä½\n")
                f.write("å»ºè®®æªæ–½:\n")
                f.write("  â€¢ æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ ‡ç­¾ä¸€è‡´æ€§\n")
                f.write("  â€¢ å°è¯•æ›´å¤§çš„æ¨¡å‹ (yolov8m)\n")
                f.write("  â€¢ å¢åŠ è®­ç»ƒæ—¶é—´å’Œæ•°æ®é‡\n")
                f.write("  â€¢ è°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨è®¾ç½®\n")
            
        print(f"ğŸ“‹ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    def run_full_evaluation(self):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ å¼€å§‹YOLOæ¨¡å‹æ·±åº¦è¯„ä¼°...")
        print("=" * 60)
        
        # è¿è¡ŒéªŒè¯
        self.run_validation()
        
        # åˆ†æé¢„æµ‹
        self.analyze_predictions()
        
        # å¤±æ•ˆåˆ†æ
        self.analyze_failure_cases()
        
        # ç”Ÿæˆå›¾è¡¨
        self.plot_pr_curves()
        self.plot_confusion_matrix()
        self.plot_confidence_distribution()
        self.visualize_prediction_samples()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_evaluation_report()
        
        print("\nâœ… æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("  - evaluation_report.txt (è¯„ä¼°æŠ¥å‘Š)")
        print("  - pr_curves.png (PRæ›²çº¿)")
        print("  - confusion_matrix.png (æ··æ·†çŸ©é˜µ)")
        print("  - confidence_distribution.png (ç½®ä¿¡åº¦åˆ†å¸ƒ)")
        print("  - prediction_samples.png (é¢„æµ‹æ ·ä¾‹)")
        
        # å¿«é€Ÿè¯Šæ–­
        precision = self.val_results.box.mp
        recall = self.val_results.box.mr
        
        print(f"\nğŸ¯ å¿«é€Ÿè¯Šæ–­:")
        print(f"   Precision: {precision:.3f}")
        print(f"   Recall: {recall:.3f}")
        
        if recall > 0.85 and precision < 0.65:
            print("   âŒ ç¡®è®¤'é«˜å¬å›ä½ç²¾åº¦'é—®é¢˜ï¼")
            print("   ğŸ’¡ ä¼˜å…ˆæ£€æŸ¥æ•°æ®æ ‡ç­¾è´¨é‡")
        else:
            print("   âœ… æŒ‡æ ‡ç›¸å¯¹å‡è¡¡")

def main():
    parser = argparse.ArgumentParser(description='YOLOæ¨¡å‹æ·±åº¦è¯„ä¼°')
    parser.add_argument('--model', required=True, help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', required=True, help='æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶')
    parser.add_argument('--conf', type=float, default=0.25, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--iou', type=float, default=0.6, help='IoUé˜ˆå€¼')
    parser.add_argument('--output-dir', default='.', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ‡æ¢åˆ°è¾“å‡ºç›®å½•
    os.chdir(args.output_dir)
    
    # è¿è¡Œè¯„ä¼°
    evaluator = ModelEvaluator(args.model, args.data, args.conf, args.iou)
    evaluator.run_full_evaluation()

if __name__ == '__main__':
    main()
