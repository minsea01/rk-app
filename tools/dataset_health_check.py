#!/usr/bin/env python3
"""YOLOæ•°æ®é›†è´¨é‡ä½“æ£€å·¥å…·.

è¯Šæ–­"å¬å›çˆ†è¡¨ã€ç²¾åº¦åä½"é—®é¢˜çš„æ•°æ®æºå¤´ã€‚

Usage:
    python tools/dataset_health_check.py --data industrial_dataset/data.yaml
    python tools/dataset_health_check.py --data data.yaml --visualize --samples 10
"""

import os
import glob
import logging
from pathlib import Path
from collections import Counter
from datetime import datetime
from typing import Dict, List, Any, Optional
import json

import cv2
import numpy as np
import yaml

# Setup logging with emoji-friendly format
logging.basicConfig(
    level=logging.INFO,
    format='%(message)s'
)
logger = logging.getLogger(__name__)


def check_dataset_health(dataset_yaml: str) -> Dict[str, Any]:
    """å®Œæ•´çš„æ•°æ®é›†å¥åº·æ£€æŸ¥.
    
    Args:
        dataset_yaml: Path to dataset YAML configuration file
        
    Returns:
        Dictionary containing health check results for each split
        
    Raises:
        FileNotFoundError: If dataset_yaml does not exist
        yaml.YAMLError: If YAML parsing fails
    """
    logger.info("ğŸ¥ YOLOæ•°æ®é›†ä½“æ£€å¼€å§‹...")
    
    # è¯»å–æ•°æ®é›†é…ç½®
    with open(dataset_yaml, 'r') as f:
        config = yaml.safe_load(f)
    
    dataset_path = Path(config['path'])
    class_names = config['names']
    num_classes = config['nc']
    
    logger.info(f"ğŸ“Š æ•°æ®é›†: {dataset_path}")
    logger.info(f"ğŸ·ï¸ ç±»åˆ«æ•°: {num_classes}")
    logger.info(f"ğŸ“ ç±»åˆ«: {class_names}")
    
    results = {}
    
    for split in ['train', 'val', 'test']:
        if split in config:
            logger.info(f"\nğŸ” æ£€æŸ¥ {split} æ•°æ®é›†...")
            
            img_dir = dataset_path / config[split].replace('/images', '').replace('images/', '') / 'images'
            label_dir = dataset_path / config[split].replace('/images', '').replace('images/', '') / 'labels'
            
            split_results = check_split_data(img_dir, label_dir, class_names, split)
            results[split] = split_results
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_health_report(results, dataset_path)
    
    return results

def check_split_data(
    img_dir: Path,
    label_dir: Path,
    class_names: List[str],
    split_name: str
) -> Dict[str, Any]:
    """æ£€æŸ¥å•ä¸ªæ•°æ®åˆ†å‰².
    
    Args:
        img_dir: Path to images directory
        label_dir: Path to labels directory
        class_names: List of class names
        split_name: Name of the split (train/val/test)
        
    Returns:
        Dictionary containing check results
    """
    if not img_dir.exists() or not label_dir.exists():
        logger.warning(f"âŒ {split_name} ç›®å½•ä¸å­˜åœ¨")
        return {}
    
    # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
    img_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png']:
        img_files.extend(glob.glob(str(img_dir / ext)))
    
    label_files = list(label_dir.glob('*.txt'))
    
    logger.info(f"ğŸ“ å›¾åƒæ–‡ä»¶: {len(img_files)}")
    logger.info(f"ğŸ“„ æ ‡ç­¾æ–‡ä»¶: {len(label_files)}")
    
    results = {
        'total_images': len(img_files),
        'total_labels': len(label_files),
        'issues': []
    }
    
    # 1. æ£€æŸ¥ç©ºæ ‡ç­¾æ–‡ä»¶
    empty_labels = []
    for label_file in label_files:
        if os.path.getsize(label_file) == 0:
            empty_labels.append(label_file)
    
    if empty_labels:
        results['issues'].append(f"ç©ºæ ‡ç­¾æ–‡ä»¶: {len(empty_labels)}ä¸ª")
        logger.warning(f"âš ï¸ å‘ç° {len(empty_labels)} ä¸ªç©ºæ ‡ç­¾æ–‡ä»¶")
        for empty_file in empty_labels[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            logger.warning(f"   - {empty_file}")
    
    # 2. æ£€æŸ¥å›¾åƒ-æ ‡ç­¾å¯¹åº”å…³ç³»
    img_stems = {Path(f).stem for f in img_files}
    label_stems = {f.stem for f in label_files}
    
    missing_labels = img_stems - label_stems
    missing_images = label_stems - img_stems
    
    if missing_labels:
        results['issues'].append(f"ç¼ºå¤±æ ‡ç­¾: {len(missing_labels)}ä¸ª")
        logger.error(f"âŒ {len(missing_labels)} ä¸ªå›¾åƒç¼ºå¤±æ ‡ç­¾")
        for missing in list(missing_labels)[:5]:
            logger.error(f"   - {missing}")
    
    if missing_images:
        results['issues'].append(f"ç¼ºå¤±å›¾åƒ: {len(missing_images)}ä¸ª")
        logger.error(f"âŒ {len(missing_images)} ä¸ªæ ‡ç­¾ç¼ºå¤±å›¾åƒ")
    
    # 3. ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
    class_counts = Counter()
    bbox_counts = Counter()  # æ¯ä¸ªå›¾åƒçš„ç›®æ ‡æ•°é‡ç»Ÿè®¡
    small_objects = 0
    large_objects = 0
    
    valid_label_files = [f for f in label_files if os.path.getsize(f) > 0]
    
    for label_file in valid_label_files:
        bbox_count = 0
        try:
            with open(label_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split()
                        if len(parts) >= 5:
                            class_id = int(parts[0])
                            x, y, w, h = map(float, parts[1:5])
                            
                            # æ£€æŸ¥æ˜¯å¦åœ¨åˆæ³•èŒƒå›´å†…
                            if 0 <= x <= 1 and 0 <= y <= 1 and 0 < w <= 1 and 0 < h <= 1:
                                if class_id < len(class_names):
                                    class_counts[class_id] += 1
                                    bbox_count += 1
                                    
                                    # ç»Ÿè®¡ç›®æ ‡å¤§å°ï¼ˆå‡è®¾å›¾åƒ640x640ï¼‰
                                    area = w * h
                                    if area < 0.01:  # å°äº1%çš„å›¾åƒé¢ç§¯
                                        small_objects += 1
                                    elif area > 0.3:  # å¤§äº30%çš„å›¾åƒé¢ç§¯
                                        large_objects += 1
                            else:
                                results['issues'].append(f"éæ³•ç±»åˆ«ID: {class_id}")
                        else:
                            results['issues'].append(f"è¾¹ç•Œæ¡†è¶Šç•Œ: {label_file}")
        
        except (IOError, OSError, ValueError) as e:
            results['issues'].append(f"æ ‡ç­¾æ–‡ä»¶è§£æé”™è¯¯: {label_file} - {str(e)}")
        
        bbox_counts[bbox_count] += 1
    
    # ç»Ÿè®¡ç»“æœ
    results['class_distribution'] = dict(class_counts)
    results['bbox_per_image'] = dict(bbox_counts)
    results['small_objects'] = small_objects
    results['large_objects'] = large_objects
    
    # 4. ç±»åˆ«ä¸å¹³è¡¡æ£€æŸ¥
    if class_counts:
        max_count = max(class_counts.values())
        min_count = min(class_counts.values())
        imbalance_ratio = max_count / max(min_count, 1)
        
        if imbalance_ratio > 10:
            results['issues'].append(f"ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡: {imbalance_ratio:.1f}:1")
            logger.warning(f"âš ï¸ ç±»åˆ«ä¸å¹³è¡¡ä¸¥é‡: {imbalance_ratio:.1f}:1")
    
    logger.info(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
    for class_id, count in sorted(class_counts.items()):
        class_name = class_names[class_id] if class_id < len(class_names) else f"class_{class_id}"
        logger.info(f"   {class_name}: {count}")
    
    logger.info(f"ğŸ¯ å°ç›®æ ‡: {small_objects}, å¤§ç›®æ ‡: {large_objects}")
    logger.info(f"ğŸ“¦ å¹³å‡æ¯å›¾ç›®æ ‡æ•°: {sum(class_counts.values()) / max(len(valid_label_files), 1):.2f}")
    
    return results

def generate_health_report(results, dataset_path):
    """ç”Ÿæˆæ•°æ®é›†å¥åº·æŠ¥å‘Š"""
    
    report_path = dataset_path / 'dataset_health_report.json'
    
    # æ€»ç»“é—®é¢˜
    all_issues = []
    for split, split_data in results.items():
        if 'issues' in split_data:
            for issue in split_data['issues']:
                all_issues.append(f"{split}: {issue}")
    
    # åˆ†æå¬å›ç‡é«˜ã€ç²¾ç¡®åº¦ä½çš„å¯èƒ½åŸå› 
    diagnosis = []
    
    # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
    for split, split_data in results.items():
        if 'class_distribution' in split_data:
            class_dist = split_data['class_distribution']
            if len(class_dist) > 0:
                max_count = max(class_dist.values())
                min_count = min(class_dist.values())
                if max_count / max(min_count, 1) > 20:
                    diagnosis.append(f"â— {split}é›†ç±»åˆ«æåº¦ä¸å¹³è¡¡ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹åå‘å¤šæ•°ç±»")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤ªå¤šå°ç›®æ ‡
                small_ratio = split_data.get('small_objects', 0) / max(sum(class_dist.values()), 1)
                if small_ratio > 0.5:
                    diagnosis.append(f"â— {split}é›†å°ç›®æ ‡è¿‡å¤š({small_ratio:.1%})ï¼Œå»ºè®®æé«˜åˆ†è¾¨ç‡")
    
    # æ£€æŸ¥æ ‡ç­¾è´¨é‡
    total_empty = sum(1 for split_data in results.values() 
                     for issue in split_data.get('issues', []) 
                     if 'ç©ºæ ‡ç­¾' in issue)
    
    if total_empty > 0:
        diagnosis.append(f"â— å‘ç°{total_empty}ä¸ªæ•°æ®åˆ†å‰²æœ‰ç©ºæ ‡ç­¾ï¼Œä¼šå¯¼è‡´FPå¢åŠ ")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_issues': len(all_issues),
            'diagnosis': diagnosis
        },
        'detailed_results': results,
        'issues': all_issues,
        'recommendations': generate_recommendations(results)
    }
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    logger.info(f"\nğŸ“‹ å¥åº·æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    # æ‰“å°è¯Šæ–­ç»“æœ
    if diagnosis:
        logger.info("\nğŸ” é—®é¢˜è¯Šæ–­:")
        for d in diagnosis:
            logger.info(f"   {d}")
    else:
        logger.info("\nâœ… æ•°æ®é›†å¥åº·çŠ¶å†µè‰¯å¥½")

def generate_recommendations(results):
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    
    recommendations = []
    
    # åŸºäºå‘ç°çš„é—®é¢˜ç»™å‡ºå»ºè®®
    for split, split_data in results.items():
        issues = split_data.get('issues', [])
        
        for issue in issues:
            if 'ç©ºæ ‡ç­¾' in issue:
                recommendations.append("åˆ é™¤ç©ºæ ‡ç­¾æ–‡ä»¶åŠå¯¹åº”å›¾åƒ")
            elif 'ç±»åˆ«ä¸å¹³è¡¡' in issue:
                recommendations.append("å¯¹å°‘æ•°ç±»è¿›è¡Œæ•°æ®å¢å¼ºæˆ–é‡é‡‡æ ·")
            elif 'ç¼ºå¤±' in issue:
                recommendations.append("ä¿®å¤å›¾åƒ-æ ‡ç­¾å¯¹åº”å…³ç³»")
            elif 'è¾¹ç•Œæ¡†è¶Šç•Œ' in issue:
                recommendations.append("ä¿®æ­£æ ‡ç­¾æ–‡ä»¶ä¸­çš„åæ ‡é”™è¯¯")
    
    # é€šç”¨å»ºè®®
    recommendations.extend([
        "ä½¿ç”¨æ›´é«˜åˆ†è¾¨ç‡è®­ç»ƒ(imgsz=960)",
        "å¢åŠ focal lossæŠ‘åˆ¶æ˜“æ ·æœ¬(fl_gamma=1.5)",
        "å¼€å¯å¼ºæ•°æ®å¢å¼º(mosaic=1.0, mixup=0.1)",
        "è°ƒæ•´confé˜ˆå€¼è¿›è¡Œæ¨ç†(0.35-0.5)"
    ])
    
    return list(set(recommendations))  # å»é‡

def visualize_sample_annotations(dataset_yaml: str, num_samples: int = 5) -> None:
    """å¯è§†åŒ–æ ·æœ¬æ ‡æ³¨è´¨é‡.
    
    Args:
        dataset_yaml: Path to dataset YAML configuration file
        num_samples: Number of sample images to visualize
        
    Raises:
        ImportError: If matplotlib is not installed
    """
    import matplotlib.pyplot as plt
    
    with open(dataset_yaml, 'r') as f:
        config = yaml.safe_load(f)
    
    dataset_path = Path(config['path'])
    
    # ä»è®­ç»ƒé›†é€‰æ‹©æ ·æœ¬
    img_dir = dataset_path / 'train' / 'images'
    label_dir = dataset_path / 'train' / 'labels'
    
    img_files = list(img_dir.glob('*.jpg'))[:num_samples]
    
    fig, axes = plt.subplots(1, len(img_files), figsize=(4*len(img_files), 4))
    if len(img_files) == 1:
        axes = [axes]
    
    for i, img_file in enumerate(img_files):
        # è¯»å–å›¾åƒ
        img = cv2.imread(str(img_file))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w = img.shape[:2]
        
        # è¯»å–æ ‡ç­¾
        label_file = label_dir / f"{img_file.stem}.txt"
        
        if label_file.exists():
            with open(label_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id, x_center, y_center, width, height = map(float, parts[:5])
                        
                        # è½¬æ¢ä¸ºåƒç´ åæ ‡
                        x1 = int((x_center - width/2) * w)
                        y1 = int((y_center - height/2) * h)
                        x2 = int((x_center + width/2) * w)
                        y2 = int((y_center + height/2) * h)
                        
                        # ç”»è¾¹ç•Œæ¡†
                        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
                        cv2.putText(img, f"C{int(class_id)}", (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
        
        axes[i].imshow(img)
        axes[i].set_title(f"Sample {i+1}")
        axes[i].axis('off')
    
    plt.tight_layout()
    plt.savefig(dataset_path / 'sample_annotations.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    logger.info(f"ğŸ“¸ æ ·æœ¬æ ‡æ³¨å¯è§†åŒ–å·²ä¿å­˜: {dataset_path}/sample_annotations.png")

def main() -> int:
    """ä¸»å‡½æ•°.
    
    Returns:
        Exit code (0 for success)
    """
    import argparse

    parser = argparse.ArgumentParser(description='YOLOæ•°æ®é›†å¥åº·æ£€æŸ¥')
    parser.add_argument('--data', type=str, required=True, help='æ•°æ®é›†YAMLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆæ ‡æ³¨å¯è§†åŒ–')
    parser.add_argument('--samples', type=int, default=5, help='å¯è§†åŒ–æ ·æœ¬æ•°é‡')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œå¥åº·æ£€æŸ¥
    results = check_dataset_health(args.data)
    
    # å¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰
    if args.visualize:
        try:
            import matplotlib.pyplot as plt
            visualize_sample_annotations(args.data, args.samples)
        except ImportError:
            logger.warning("âš ï¸ å¯è§†åŒ–éœ€è¦matplotlibï¼Œè¯·å®‰è£…: pip install matplotlib")
        except (IOError, OSError, cv2.error) as e:
            logger.warning(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
    
    logger.info("\nğŸ‰ æ•°æ®é›†ä½“æ£€å®Œæˆï¼")
    return 0

if __name__ == "__main__":
    main()