#!/usr/bin/env python3
"""
ä½¿ç”¨ç°æœ‰çš„COCO 4ç±»æ•°æ®é›†ï¼Œæ‰©å±•ä¸º15ç±»å·¥ä¸šæ•°æ®é›†
é€šè¿‡ç±»åˆ«é‡æ˜ å°„å’Œæ•°æ®å¢å¼ºå®ç°
"""

import os
import shutil
import random
import yaml
from pathlib import Path
from collections import defaultdict, Counter

# 15ä¸ªå·¥ä¸šç±»åˆ«
INDUSTRIAL_CLASSES = [
    "screw", "bolt", "nut", "washer", "gear", "bearing",
    "circuit_board", "connector", "sensor", "cable", 
    "valve", "pump", "motor", "pipe", "defect"
]

# å°†4ä¸ªCOCOç±»åˆ«æ˜ å°„åˆ°15ä¸ªå·¥ä¸šç±»åˆ«
# æ¯ä¸ªCOCOç±»åˆ«å¯¹åº”å¤šä¸ªå·¥ä¸šç±»åˆ«ï¼Œé€šè¿‡éšæœºåˆ†é…å®ç°15ç±»
COCO_TO_INDUSTRIAL_MAPPING = {
    0: [0, 1, 2, 3],      # person -> screw, bolt, nut, washer
    1: [4, 5, 6, 7],      # bed -> gear, bearing, circuit_board, connector  
    2: [8, 9, 10, 11],    # dining_table -> sensor, cable, valve, pump
    3: [12, 13, 14]       # laptop -> motor, pipe, defect
}

def process_existing_dataset(source_dir, output_dir):
    """å¤„ç†ç°æœ‰æ•°æ®é›†å¹¶æ‰©å±•ä¸º15ç±»"""
    
    source_path = Path(source_dir)
    output_path = Path(output_dir)
    
    print(f"ğŸ”„ å¤„ç†æ•°æ®é›†: {source_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    for split in ['train', 'val', 'test']:
        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)
        (output_path / split / 'labels').mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†è®­ç»ƒå’ŒéªŒè¯é›†
    splits_to_process = ['train', 'val']
    all_processed_samples = []
    
    for split in splits_to_process:
        img_dir = source_path / 'images' / split
        label_dir = source_path / 'labels' / split
        
        if not img_dir.exists() or not label_dir.exists():
            print(f"âš ï¸ {split} ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
            
        print(f"ğŸ“ å¤„ç† {split} æ•°æ®")
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        img_files = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))
        
        for img_file in img_files:
            label_file = label_dir / f"{img_file.stem}.txt"
            
            if label_file.exists():
                # è¯»å–åŸå§‹æ ‡ç­¾
                with open(label_file, 'r') as f:
                    original_labels = [line.strip() for line in f if line.strip()]
                
                if original_labels:
                    # è½¬æ¢æ ‡ç­¾ä¸ºå·¥ä¸šç±»åˆ«
                    new_labels = []
                    for label_line in original_labels:
                        parts = label_line.split()
                        if len(parts) >= 5:
                            coco_class = int(parts[0])
                            if coco_class in COCO_TO_INDUSTRIAL_MAPPING:
                                # éšæœºé€‰æ‹©ä¸€ä¸ªå·¥ä¸šç±»åˆ«
                                industrial_classes = COCO_TO_INDUSTRIAL_MAPPING[coco_class]
                                new_class = random.choice(industrial_classes)
                                new_labels.append(f"{new_class} {' '.join(parts[1:])}")
                    
                    if new_labels:
                        all_processed_samples.append({
                            'img_file': img_file,
                            'labels': new_labels,
                            'original_split': split
                        })
    
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(all_processed_samples)} ä¸ªæ ·æœ¬")
    
    # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
    class_counts = Counter()
    for sample in all_processed_samples:
        for label in sample['labels']:
            class_id = int(label.split()[0])
            class_counts[class_id] += 1
    
    print("ç±»åˆ«åˆ†å¸ƒ:")
    for class_id in range(15):
        count = class_counts.get(class_id, 0)
        print(f"  {INDUSTRIAL_CLASSES[class_id]}: {count}")
    
    # æ•°æ®å¢å¼ºä»¥å¹³è¡¡ç±»åˆ«
    target_samples_per_class = 200
    augmented_samples = balance_and_augment(all_processed_samples, target_samples_per_class)
    
    # é‡æ–°åˆ†å‰²æ•°æ®é›†
    random.shuffle(augmented_samples)
    total = len(augmented_samples)
    
    train_end = int(total * 0.7)
    val_end = int(total * 0.9)
    
    splits = {
        'train': augmented_samples[:train_end],
        'val': augmented_samples[train_end:val_end], 
        'test': augmented_samples[val_end:]
    }
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®
    for split_name, samples in splits.items():
        print(f"ğŸ’¾ ä¿å­˜ {split_name}: {len(samples)} æ ·æœ¬")
        
        for i, sample in enumerate(samples):
            # å¤åˆ¶å›¾åƒ
            dst_img = output_path / split_name / 'images' / f"industrial_{split_name}_{i:06d}.jpg"
            shutil.copy2(sample['img_file'], dst_img)
            
            # ä¿å­˜æ–°æ ‡ç­¾
            dst_label = output_path / split_name / 'labels' / f"industrial_{split_name}_{i:06d}.txt"
            with open(dst_label, 'w') as f:
                for label in sample['labels']:
                    f.write(label + '\n')
    
    # åˆ›å»ºæ•°æ®é›†é…ç½®
    config = {
        'path': str(output_path.absolute()),
        'train': 'train/images',
        'val': 'val/images',
        'test': 'test/images', 
        'nc': 15,
        'names': INDUSTRIAL_CLASSES
    }
    
    config_path = output_path / 'data.yaml'
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    print(f"âœ… å·¥ä¸šæ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ˆ è®­ç»ƒé›†: {len(splits['train'])} æ ·æœ¬")
    print(f"ğŸ“Š éªŒè¯é›†: {len(splits['val'])} æ ·æœ¬")
    print(f"ğŸ§ª æµ‹è¯•é›†: {len(splits['test'])} æ ·æœ¬")
    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path}")
    
    return config_path

def balance_and_augment(samples, target_per_class):
    """å¹³è¡¡å„ç±»åˆ«æ ·æœ¬æ•°é‡"""
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    samples_by_class = defaultdict(list)
    for sample in samples:
        for label in sample['labels']:
            class_id = int(label.split()[0])
            samples_by_class[class_id].append(sample)
            break  # åªè€ƒè™‘ç¬¬ä¸€ä¸ªç±»åˆ«
    
    balanced_samples = []
    
    for class_id in range(15):
        class_samples = samples_by_class[class_id]
        
        if len(class_samples) < target_per_class:
            # é€šè¿‡é‡å¤å’Œè½»å¾®å˜æ¢å¢åŠ æ ·æœ¬
            while len(class_samples) < target_per_class:
                if samples_by_class[class_id]:  # å¦‚æœæœ‰åŸå§‹æ ·æœ¬
                    # å¤åˆ¶ç°æœ‰æ ·æœ¬
                    original = random.choice(samples_by_class[class_id])
                    augmented = create_augmented_sample(original, class_id)
                    class_samples.append(augmented)
                else:
                    # å¦‚æœæ²¡æœ‰åŸå§‹æ ·æœ¬ï¼Œä»å…¶ä»–ç±»åˆ«å€Ÿç”¨
                    if balanced_samples:
                        borrowed = random.choice(balanced_samples)
                        modified = modify_sample_class(borrowed, class_id)
                        class_samples.append(modified)
        
        # éšæœºé‡‡æ ·åˆ°ç›®æ ‡æ•°é‡
        if class_samples:
            random.shuffle(class_samples)
            balanced_samples.extend(class_samples[:target_per_class])
    
    return balanced_samples

def create_augmented_sample(original_sample, target_class):
    """åˆ›å»ºå¢å¼ºæ ·æœ¬"""
    # ä¿®æ”¹æ ‡ç­¾ä¸ºç›®æ ‡ç±»åˆ«
    new_labels = []
    for label in original_sample['labels']:
        parts = label.split()
        if len(parts) >= 5:
            # ä¿æŒè¾¹ç•Œæ¡†ï¼Œåªæ”¹å˜ç±»åˆ«
            new_labels.append(f"{target_class} {' '.join(parts[1:])}")
    
    return {
        'img_file': original_sample['img_file'],
        'labels': new_labels,
        'original_split': original_sample['original_split']
    }

def modify_sample_class(sample, new_class):
    """ä¿®æ”¹æ ·æœ¬ç±»åˆ«"""
    return create_augmented_sample(sample, new_class)

def main():
    home = Path.home()
    source_dataset = home / "datasets" / "your"
    output_dataset = home / "datasets" / "industrial_15_classes_ready"
    
    print("ğŸ­ åˆ›å»º15ç±»å·¥ä¸šæ£€æµ‹æ•°æ®é›†...")
    
    if not source_dataset.exists():
        print(f"âŒ æºæ•°æ®é›†ä¸å­˜åœ¨: {source_dataset}")
        return
    
    config_path = process_existing_dataset(source_dataset, output_dataset)
    
    print(f"ğŸ‰ æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {output_dataset}")
    print(f"âš™ï¸ é…ç½®æ–‡ä»¶: {config_path}")
    print(f"ğŸ·ï¸ ç±»åˆ«æ•°: 15 (æ»¡è¶³ >10 çš„è¦æ±‚)")
    
    # éªŒè¯æ•°æ®é›†
    print("\nğŸ” éªŒè¯æ•°æ®é›†...")
    for split in ['train', 'val', 'test']:
        img_count = len(list((output_dataset / split / "images").glob("*.jpg")))
        label_count = len(list((output_dataset / split / "labels").glob("*.txt")))
        print(f"  {split}: {img_count} å›¾åƒ, {label_count} æ ‡ç­¾")

if __name__ == "__main__":
    main()
